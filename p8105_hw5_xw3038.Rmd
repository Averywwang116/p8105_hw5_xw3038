---
title: "p8105_hw5_xw3038"
author: "Avery Wang"
date: "2024-11-14"
output: github_document
---

```{r}
library(tidyverse)
library(rvest)
library(readr)
library(purrr)
library(tidyr)
library(broom)

set.seed(1)

```

## Problem 1

```{r}
#function for finding the duplicate
check_duplicate_birthday=function(n){
  birthdays = sample(1:365, n, replace =TRUE)
  #check if they are duplicated
  if(length(birthdays)==length(unique(birthdays))){
    return(FALSE)
  }else{
    return(TRUE)
  }
  
}


```

```{r}
probability_calculation=
  expand_grid(
    #sample size from 2 to 50
    sample_size=2:50,
    iter=1:10000) |>
  mutate(
    #using map function
    duplicate_df=map(sample_size,check_duplicate_birthday)
  )|> 
  #unnest
  unnest(duplicate_df) |>
  group_by(sample_size)|>
  summarize(probability = mean(duplicate_df))
probability_calculation
```
```{r}
#plot the probability
probability_calculation|>
  ggplot(aes(x=sample_size,y=probability))+
  geom_line() +
  geom_point() +
  labs(
    x = "Group Size",
    y = "Probability of Shared Birthday",
    title = "Probability of At Least Two People Sharing a Birthday"
  ) +
  theme_minimal()
```

With the group size getting larger, the probability that at least two people in the group share the same birthday also increases

# Problem 2

```{r}
# make simulation
simulation_ttest=function(mu,n = 30,sigma = 5,alpha=0.05){
  x =rnorm(n, mean = mu, sd = sigma)
  t_test =t.test(x, mu=0)
  tidy_test=broom::tidy(t_test)
  tibble(
      mu_hat = tidy_test$estimate,
      p_value = tidy_test$p.value
    )
}
#for mu is 0
ttest_df_0=expand_grid(dataset=1: 5000,mu=0)|> 
  mutate(
    estimate_df = map_dfr(mu, simulation_ttest)
  ) |> 
  unnest(estimate_df)

# for the other case
ttest_df_others=expand_grid(dataset=1: 5000,mu=1:6)|> 
  mutate(
    estimate_df = map_dfr(mu, simulation_ttest)
  ) |> 
  unnest(estimate_df)

```

```{r}
#combine in order to plot
combined_df=bind_rows(ttest_df_0,ttest_df_others)
```

```{r}
#proportion of times rejected
combined_df=combined_df |>
  mutate(
    rejection=if_else(p_value<0.05,TRUE,FALSE)
  )

#group by mu and then plot
combined_df|> group_by(mu)|>
  summarize(proportion_rejection=mean(rejection)) |>
  ggplot(aes(x=mu,y=proportion_rejection))+
  geom_line() +
  geom_point() +
  labs(
    x = "mu",
    y = "power of test",
    title = "Proportion of Times Null was Rejected with Different mu"
  ) +
  theme_minimal()

```

It can be observed that the larger effect size, the higher power


```{r}
combined_df|> group_by(mu)|>
  summarize(
    all_data= mean(mu_hat),
    reject_null = mean(mu_hat[p_value < 0.05])
  )|>
  pivot_longer(
    cols = c(all_data, reject_null),
    names_to = "estimate_type",
    values_to = "estimate"
  ) |>
  ggplot(aes(x = mu, y = estimate, color = estimate_type)) +
  geom_line() +
  geom_point() +
  labs(
    x = "Mu",
    y = "Average Estimate",
    title = "Average Estimate of estimate for All and Rejected Null",
    color = "Estimate Type"
  ) +
  theme_minimal()
  
```

the sample average of $\hat{\mu}$ for which the null hypothesis was rejected is not approximately equal to the true value of $\mu$. The possible reason is that samples with higher average estimate are more likely to have statistically significant results, and thus more likely to reject the null hypothesis.

## Problem 3

```{r}
homicide=read_csv(file = "./homicide-data.csv", na = c(".", "NA", "")) |>
  janitor::clean_names() 
```
The raw dataset has `r ncol(homicide)` columns and `r nrow(homicide)` rows. And the dataset include variables: `r unique(names(homicide))` that describes the information about the homicide cases such as the victim information, location of the homicide events, date being reported, and the disposition of the homicide case.

* summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
#Create a city_state
homicide_data=homicide|> 
  mutate(city_state = paste(city, state, sep = ", "))|>
  distinct()
homicide_data|>
  group_by(city)|>
  #find the total homicide 
  summarize(total_homicides = n(),
            # find the homocides that are unsolved
            unsolved_homicides=sum(disposition %in% c("Closed without arrest", "Open/No arrest")))

```


* Estimate the proportion of homicides that are unsolved for Baltimore, MD

```{r}
#filter out Baltimore
baltimore_data =homicide_data|>filter(city_state=="Baltimore, MD")
total_bal=nrow(baltimore_data)
unsolved_bal=sum(baltimore_data$disposition %in% c("Closed without arrest", "Open/No arrest"))
# Perform the proportion test
prop_test_result=prop.test(unsolved_bal, total_bal)
tidy_result=broom::tidy(prop_test_result)

estimated_proportion=tidy_result|>pull(estimate)
conf_low=tidy_result|>pull(conf.low)
conf_high=tidy_result|>pull(conf.high)

```

So the estimated proportion is `r round(estimated_proportion, 2)` and the 95% percent CI falls in [`r round(conf_low, 2)`, `r round(conf_high, 2)`].

* Extract both the proportion of unsolved homicides and the confidence interval for each

```{r}
#make summary on the homocide data for total and unsolved
homicide_summary_city =homicide_data |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides=sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
  )

#figure out the proportion and CI
cities_proportion=homicide_summary_city |>
  mutate(
    proportion_cities= map2(
      unsolved_homicides, total_homicides,
      ~ prop.test(.x,.y) |>broom::tidy()
    )
  )|>unnest(proportion_cities)|> 
  select(city_state,total_homicides, unsolved_homicides,estimate, conf.low,conf.high)

cities_proportion
```

* Create a plot that shows the estimates and CIs for each city

```{r}
cities_proportion|>
  ggplot(aes(x = reorder(city_state, estimate), y = estimate))+
  geom_point(color="red",stat="identity")+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))+
  coord_flip() +
  labs(
    title = "Proportion and CIs of Unsolved Homicides by City",
    x = "City",
    y = "Proportion and 95% CI"
  ) +theme_minimal() 

```

